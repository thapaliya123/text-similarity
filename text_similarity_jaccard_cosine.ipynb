{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Similarity\n",
    "- There is always need to compute the similarity in meaning between texts.\n",
    "    1. **Search Engines** need to model the relevance (appropriate) of a document to a query, beyond the overlap in words between the two.\n",
    "        - For instance, **question-and-answer** sites such as Quora or stackoverflow need to determine whether a question has already been asked before.\n",
    "    2. **In legal matters**, text similarity task allow to mitigate risks on a new contract, based on the assumption that if a new contract is similar to a existent one that has been proved to be resilient, the risk of this new contract being the cause of financial loss is minimised.\n",
    "    3. **In customer services**, AI system should be able to understand semantically similar queries from users and provide a uniform response.\n",
    "        - For example, if the user asks **What has happened to my delivery?** or **What is wrong with my shipping?**, the user will expect the same response.\n",
    "- All the similarity measures map to the range of [-1, 1] or [0, 1]. The 0 or -1 represents minimal similarity, and 1 represents absolute similarity\n",
    "- Text similarity has to determine how 'close' two piece of text are both in surface closeness (**lexical similarity**) and meaning (**semantic similarity**)\n",
    "    - **lexical similarity** of 1 would mean a total overlap between vocabularies, whereas 0 means there are no common words.\n",
    "    - **semantic similarity** between two text is based on the likeness of their meaning\n",
    "- For instance, how similar are the phrases **the cat ate the mouse** with the **the mouse ate the cat food** by just looking at the words?\n",
    "    - **On the surface**, if you consider only **word level similarity**, these two phrases appear very similar as 3 of the 4 unique words are an exact overlap. \n",
    "        - It typcally does not take into account the actual meaning behind words or the entire phrase in context.\n",
    "    - Instead of doing a **word for word comparison**, we also need to pay attention to **context** in order to capture more of the **semantics**.\n",
    "        - To consider **semantic similarity** we need to focus on **phrase/paragraph levels** (or lexical chain level) where piece of text is broken into a relevant group of related words prior to computing similarity. We know that while the words significantly overlap, these two phrases actually have different meaning\n",
    "- There is a **dependency structure** in any sentences:\n",
    "    > **mouse** is the object of **ate** in the first case and **food** is the object of **ate** in the second case\n",
    "    - Since **differences in word order** often go hand in hand with **differences in meaning** (compare **the dog bites the man** with **the man bites the dog**), we'd like our sentence embeddings to be **sensitive to this variation**.\n",
    "    \n",
    "\n",
    "### Computing Text Similarity\n",
    "- In this notebook we deal with mainly two types of text similarity measure,\n",
    "   1. **Jaccard Similarity**\n",
    "   2. **Cosine Similarity**\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "- Text preprocessing steps are very essential before we compute similarity scores using technique mentioned\n",
    "- As we know text is represented as a vector in multidimension space. In vector space model each word/term is an axis or dimension, the number of unique words means the number of dimensions, hence text preprocessing should done before computing any similarity  measure\n",
    "- **Following preprocessing techniques are applied:**\n",
    "   1. **lower casing**\n",
    "   2. **remove special symbols**\n",
    "   3. **remove punctuation**\n",
    "   4. **remove stopwords**\n",
    "   5. **Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necssary packages\n",
    "import spacy\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# load the english model and initialize an object called 'nlp'\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# convert string into its lower case\n",
    "def lower_casing(text):\n",
    "    \"\"\"\n",
    "    Obtained the lower case version and passed lower case version of text \n",
    "\n",
    "    Arguments:\n",
    "    text: raw text, strings\n",
    "\n",
    "    Returns:\n",
    "    lower_text: string, representing lower case of raw text\n",
    "    \"\"\"\n",
    "\n",
    "    # lower casing\n",
    "    lower_text = text.lower()\n",
    "\n",
    "    return lower_text\n",
    "\n",
    "# remove special symbols using regex\n",
    "def remove_special_symbols(text):\n",
    "    \"\"\"\n",
    "    remove specified symbols such as numbers, replace hyphen and dot with space in given text passed as an argument \n",
    "    \n",
    "    Arguments: raw text, strings\n",
    "    \n",
    "    Returns: \n",
    "    processed_text = text obtained after preprocessing\n",
    "    \"\"\"\n",
    "    \n",
    "    # replace hyphen, dot with space\n",
    "    text_with_no_hyphen = text.replace(\"-\", \" \").replace(\".\", \" \")\n",
    "    \n",
    "\n",
    "    #specifying the things we want to remove using regular expressions and removing using sub\n",
    "    regex_remove = \"([0-9])|(@[A-Za-z0-9]+)|([^0-9A-Za-z t])|(w+://S+)|^RT|http.+?\"\n",
    "    \n",
    "    clean_text = re.sub(regex_remove, '', text_with_no_hyphen).strip()\n",
    "    clean_text = ''.join(clean_text)\n",
    "    \n",
    "\n",
    "    \n",
    "    return clean_text\n",
    "    \n",
    "# remove punctuations\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"\n",
    "    removes punctuation symbols present in the raw text passed as an arguments\n",
    "    \n",
    "    Arguments:\n",
    "    text: raw text\n",
    "    \n",
    "    Returns: \n",
    "    not_punctuation: list of tokens without punctuation\n",
    "    \"\"\"\n",
    "    # passing the text to nlp and initialize an object called 'doc'\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    not_punctuation = []\n",
    "    # remove the puctuation\n",
    "    for token in doc:\n",
    "        if token.is_punct == False:\n",
    "            not_punctuation.append(token)\n",
    "    \n",
    "    return not_punctuation\n",
    "\n",
    "\n",
    "# tokenize words\n",
    "def tokenize_word(text):\n",
    "    \"\"\"\n",
    "    Tokenize the text passed as an arguments into a list of words(tokens)\n",
    "    \n",
    "    Arguments:\n",
    "    text: raw text\n",
    "    \n",
    "    Returns:\n",
    "    words: list containing tokens in text\n",
    "    \"\"\"\n",
    "    # passing the text to nlp and initialize an object called 'doc'\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Tokenize the doc using token.text attribute\n",
    "    words = [token.text for token in doc]\n",
    "        \n",
    "    # return list of tokens\n",
    "    return words\n",
    "\n",
    "# remove stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    Removes stopwords passed from the tokens list passed as an arguments\n",
    "    \n",
    "    Arguments:\n",
    "    tokens: list of tokens\n",
    "    \n",
    "    Returns:\n",
    "    tokens_without_sw: list of tokens of raw text without stopwords\n",
    "    \"\"\"\n",
    "    \n",
    "    # getting list of default stop words in spaCy english model\n",
    "    stopwords =nlp.Defaults.stop_words\n",
    "    \n",
    "    # tokenize text\n",
    "    text_tokens = tokens\n",
    "    \n",
    "    # remove stop words:\n",
    "    tokens_without_sw = [word for word in text_tokens if word not in stopwords]\n",
    "    \n",
    "    # return list of tokens with no stop words\n",
    "    return tokens_without_sw\n",
    "\n",
    "\n",
    "# Lemmatization\n",
    "def lemmatization(tokens):\n",
    "    \"\"\"\n",
    "    obtain the lemma of the each token in the token list, append to the list, and returns the list\n",
    "    \n",
    "    Arguments:\n",
    "    text: list of tokens\n",
    "    \n",
    "    Returns:\n",
    "    lemma_list: return list of lemma corresponding to each tokens\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    lemma_list = []\n",
    "    # Lemmatization\n",
    "    for token in tokens:\n",
    "        lemma_list.append(token.lemma_)\n",
    "    \n",
    "    return lemma_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets combine above defined different preprocessing steps into single module named **preprocess_text(text)** which will be used later to preprocess the document before calculating similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw text preprocessing\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    - preprocess raw text passed as an arguments\n",
    "    - preprocessing of text includes, lower_casing, remove_special_symbols, remove_punctuation, remove_stopwords, lemmatization\n",
    "    \n",
    "    Arguments: raw text, string\n",
    "    \n",
    "    Returns: list of tokens obtained after preprocessing\n",
    "    \"\"\"\n",
    "    \n",
    "    # lower casing\n",
    "    lower_case_text = lower_casing(text)\n",
    "    \n",
    "    # remove special symbols\n",
    "    removed_special_symbols = remove_special_symbols(lower_case_text)\n",
    "    \n",
    "    # remove punctuation\n",
    "    tokens_without_punct = remove_punctuation(removed_special_symbols)\n",
    "    \n",
    "    # remove stopwords\n",
    "    tokens_without_stopwords = remove_stopwords(tokens_without_punct)\n",
    "    \n",
    "    # lemmatization\n",
    "    lemma_of_tokens = lemmatization(tokens_without_stopwords)\n",
    "\n",
    "    # return preprocessed text\n",
    "    return lemma_of_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard Similarity \n",
    "- Jaccard coefficient is used to compare the similarity and non-similarity of two documents based on the presence or absence of terms in documents.\n",
    "- Jaccard similarity or intersection over union is defined as **size of intersection divided by size of union of two sets**.\n",
    "- **Jaccard(A, B):**  _intersection(A, B)/union(A, B)_\n",
    "- The **degree of similarity** is a value between 0 and 1.\n",
    "   - If the **degree of similarity = 1** the two documents are same and identical.\n",
    "   - If the **degree of similarity = 0** the two documents are completely dissimilar.\n",
    "\n",
    "### Cosine Similarity\n",
    "- Cosine similarity is a metric used to measure how similar the documents are irrespective of their size.\n",
    "- Mathematically, it measures the **cosine of the angle** between two vectors project in multidimensional space.\n",
    "- The cosine similarity is advantageous because even if the two similar documents are far apart by the Euclidean distance (due to the size of the document), chances are they may still oriented closer together. The small the angle, higher the cosine similarity.\n",
    "- The **cosine similarity of A and B is defined as,**\n",
    "    - **cos(A, B)** = _dot_product(A,B) / magnitude(A) * magnitude(B)_\n",
    "- The **degree of similarity** is a value between -1 and 1.\n",
    "    - If documents are similar, their vectors will be in the same direction from origin, thus, they form a relatively small angle, which its cosine value will be near 1.\n",
    "    - On the other hand, when two vectors are different direction from origin, they form a wide angle and the value of the cosine is near to -1.\n",
    "- **Important Property**\n",
    "    - Easy to implement\n",
    "    - Efficient to evaluate\n",
    "    - it is independence from the length of documents.\n",
    "    \n",
    "Now, lets define the function for **jaccard_similarity(text1, text2)** and **cosine_similarity(df)** to calculate similarity score using two different technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calulate jaccard similarity\n",
    "def jaccard_similarity(text1, text2):\n",
    "    \"\"\"\n",
    "    calculate jaccard similarity index between two preprocessed tokenized list of text i.e. text1, text2\n",
    "    jaccard index = size of intersection / size of union\n",
    "\n",
    "    Arguments:\n",
    "    text1: article or text which is preprocessed and contains list of tokens, (list of tokens)\n",
    "    text2: article or text which is preprocesed and contains list of tokens, (list of tokens)\n",
    "    \n",
    "    Returns:\n",
    "    jaccard_score: result of similarity between two text using jacard index\n",
    "    \"\"\"\n",
    "    # intersection between two text i.e. text1, text2\n",
    "    intersection = set(text1).intersection(set(text2))\n",
    "    \n",
    "    # union between two text i.e. text1, text2\n",
    "    union = set(text1).union(set(text2))    \n",
    "    \n",
    "    # calculate jaccard score\n",
    "    jaccard_score = len(intersection)/len(union)\n",
    "    \n",
    "\n",
    "    return jaccard_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have written module that computes **jaccard similarity** between two documents\n",
    "\n",
    "Now lets, move towards **cosine similarity**\n",
    "\n",
    "#### Term Document  Matrix\n",
    "- **Term Document Matrix** contains frequency of words for each documents represented using pandas dataframe\n",
    "- **Why term document matrix?**\n",
    "     - Text data are considered to be an unstructured data, which does not have a specific format and cannot be analyzed on its own. \n",
    "     - The text data has has to be converted into a structured format for further analysis.\n",
    "     - To form this conversion **term document matrix** is deployed.\n",
    "- **Term Document Matrix** is a two dimension matrix which its columns corresponds to documents, and its rows correspond to terms. The value of each cell is the term frequency of a word in a particular document.\n",
    "\n",
    "#### Comparison between Term Document Matrix and Jaccard \n",
    "- The total number of rows in **Term Document Matrix** which represents the terms available in both documents are considered as the **union of two documents** in Jaccard Similarity .\n",
    "- Likewise, the total number of words that are remained after eliminating the terms that are having value zero in any of two documents returns the **intersection of two documents**.\n",
    "\n",
    "Now, let's **obtain the term document** matrix and using **pandas dataframe** and use it compute **cosine similarity** between the two documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_term_document(text1, text2):\n",
    "    \"\"\"\n",
    "    Takes two preprocessed tokenized text i.e. text1 and text2 and calculate the term in each documents and return\n",
    "    pandas dataframe representing frequency of union of words in text1 and text2 for each documents i.e. text1 and text2\n",
    "    in normalized form\n",
    "    \n",
    "    Arguments:\n",
    "    text1: Preprocessed tokenized text or documents, list of tokens\n",
    "    text2: Preprocessed tokenized text or documents, list of tokens\n",
    "    \n",
    "    Returns:\n",
    "    df: pandas dataframe containing frequency of word terms for each documents in normalize form\n",
    "    \"\"\"\n",
    "    # union of word tokens in text1 and text2\n",
    "    union_of_words = set(text1).union(set(text2))\n",
    "    \n",
    "    word_count_text1 = [] #initialeze for sotring frequency of words\n",
    "    word_count_text2 = [] #initialize for storing frequency of words\n",
    "    \n",
    "    # word frequency count for each text\n",
    "    for word in union_of_words:\n",
    "        word_count_text1.append(text1.count(word))\n",
    "        word_count_text2.append(text2.count(word))\n",
    "    \n",
    "    # create dataframe containing frequency of words for each text i.e. text1 and text2\n",
    "    df = pd.DataFrame(list(zip(word_count_text1, word_count_text2)), index=union_of_words, columns=['text1', 'text2'])\n",
    "    \n",
    "    # normalize each text\n",
    "    df[\"text1\"] = round(df[\"text1\"]/df[\"text1\"].sum(), 3)\n",
    "    df[\"text2\"] = round(df[\"text2\"]/df[\"text2\"].sum(), 3)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def cosine_similarity(dataframe):\n",
    "    \"\"\"\n",
    "    computes cosine of the angle between two vectors and helps us to find out how related two documents are\n",
    "    \n",
    "    Arguments:\n",
    "    dataframe: dataframe containing normalized terms/words in each document\n",
    "    \n",
    "    Returns:\n",
    "    cosine_similarity_score: cosine_similarity_score between two documents\n",
    "    \"\"\"\n",
    "    # convert each column of dataframe which represent individual documents into array\n",
    "    document_one_vector = np.array(df.iloc[:, 0]) # vector representing document1 or text1\n",
    "    document_two_vector = np.array(df.iloc[:, 1]) # vector representing document2 or text2\n",
    "    \n",
    "    # calulate dot product of the two vector\n",
    "    dot_product_of_two_vector = np.dot(document_one_vector, document_two_vector)\n",
    "    \n",
    "    # calculate magnitude for each vector\n",
    "    document_one_vector_magnitude = np.sqrt(np.sum(np.square(document_one_vector))) # magnitude for vector document1\n",
    "    document_two_vector_magnitude = np.sqrt(np.sum(np.square(document_two_vector))) # magnitude for vector document2\n",
    "    \n",
    "    # compute cosine similarity for two documents\n",
    "    cosine_similarity_score = dot_product_of_two_vector/(document_one_vector_magnitude*document_two_vector_magnitude)\n",
    "    \n",
    "    return cosine_similarity_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally necessary steps for two similarity measure technique i.e. **jaccard_similarity** and **cosine_similarity** is completed.  \n",
    "\n",
    "Now, lets use all the defined modules, staring from **text preprocessing** to **cosine_similarity** to calculate the **similarity score** between two documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>jaccard_similarity</th>\n",
       "      <td>0.288462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cosine_similarity</th>\n",
       "      <td>0.593076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    similarity_score\n",
       "jaccard_similarity          0.288462\n",
       "cosine_similarity           0.593076"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sample article 1\n",
    "article_1 = \"\"\"Nepal on Thursday reported 12 more Covid-19-related fatalities, pushing the death toll to 1,663. \n",
    "The country also recorded 1,217 new cases.The overall infection tally has reached 245,650 with 12,386 active cases.\n",
    "According to the Ministry of Health and Population, 231,601 infected people have recovered from the disease so far; \n",
    "1,064 of them in the past 24 hours.\"\"\"\n",
    "\n",
    "# sample article 2\n",
    "article_2 = \"\"\"As of Thursday, the number of confirmed cases in the Valley has reached 114,409 While Kathmandu has\n",
    "reported 432 Covid-19-related fatalities so far, Lalitpur and Bhaktapur have recorded 129 and 97 deaths respectively.\"\"\"\n",
    "\n",
    "# sample article 3\n",
    "article_3 = \"\"\"The Johns Hopkins University data dashboard reported 3,124 deaths, breaking a record of 2,885 set \n",
    "just last week. New infections are also booming, and across the nation hospitals are running out of beds, prompting stay-at-home orders in some places and mask mandates in 38 states.\"\"\"\n",
    "\n",
    "# preprocess the text\n",
    "preprocessed_article_1 = [item for item in preprocess_text(article_1) if item not in [' ', '  ']]\n",
    "preprocessed_article_2 = [item for item in preprocess_text(article_2) if item not in [' ', '  ']]\n",
    "\n",
    "# compute jaccard similarity socre\n",
    "jaccard_similarity_score = jaccard_similarity(preprocessed_article_1, preprocessed_article_2)\n",
    "\n",
    "# compute cosine similarity score\n",
    "df = obtain_term_document(preprocessed_article_1, preprocessed_article_2)\n",
    "cosine_similarity_score = cosine_similarity(df)\n",
    "\n",
    "# add jaccard_similarity_score and cosine_similarity_score to dataframe\n",
    "data = {'similarity_score':[jaccard_similarity_score, cosine_similarity_score]}\n",
    "similarity_score = pd.DataFrame(data, index = ['jaccard_similarity', 'cosine_similarity'])\n",
    "\n",
    "# plot similarity score\n",
    "similarity_score.plot(kind=\"barh\")\n",
    "\n",
    "# similarity score represented using pandas dataframe\n",
    "similarity_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see we have obtained the **jaccard_similarity_score** and **cosine_similarity_score** for two news article \n",
    "- According to **jaccard_similarity_score** the text are not quite similar, with **similarity_score=0.288462**\n",
    "- According to **cosine_similarity_score** the text are more  similar as compared to **jaccard_similarity_score**, with **similarity_score=0.593076**\n",
    "- Actually, two text are a bit similar as both are talking about _covid-19_, so **cosine_similarity** is able to give the better **similarity score.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "- We discussed importance of text similarity and two methods in computing similarity score between two documents.\n",
    "- Text similarity measures are very important concepts in text mining and information retrieval that helps to quantify similarity between documents, which is effective in improving the performance of search engines and browsing techniques.\n",
    "- Similarity measurement are widely used in applications such as **duplicate detection**, **automatic scoring**, **topic detection**, **document clustering**, **text classification.**\n",
    "- All the similarity measures map to the range of [-1, 1] or [0, 1]. The 0 or -1 represents minimal similarity, and 1 represents absolute similarity.\n",
    "- The big idea of calculating similarity is that **documents is represented as vectors of features**, and **compare documents by measuring the distance between these features.**\n",
    "- Before computing **similarity score** between two texts or documents, they are **preprocessed** using technique mentioned below in order:\n",
    "   - **lower casing**\n",
    "   - **remove special symbols**\n",
    "   - **remove punctuation**\n",
    "   - **remove stopwords**\n",
    "   - **Lemmatization**\n",
    "- Here **jaccard similarity** and **cosine similarity** are used to compute the similarity score between two text or documents.\n",
    "- **jaccard similarity** computes the similarity score by **size of intersection divided by size of union of two sets of words**\n",
    "- **Cosine similarity** measures the **cosine of the angle** between two vectors projected in multidimensional space using formulae\n",
    "   - **cos(A, B)** = _dot_product(A,B) / magnitude(A) * magnitude(B)_\n",
    "- Jaccard similarity may be inefficient if:\n",
    "   - two similar documents are small, so that there may not be common words leading to least similar score\n",
    "   - the size of the document increases, the number of common words tend to increase even if the documents talk about different topics i.e. **dissimilar documents**\n",
    "- **advantage of cosine similarity**\n",
    "   - cosine similarity measures text similarity by projecting documents in space, and calculating cosine angle between them.\n",
    "   - These approach is independent from the length of documents, which result better similarity scores.\n",
    "- **Future work**\n",
    "   - Research on other different similarity measures such as:\n",
    "      - **Soft Cosine Similarity**\n",
    "      - **knowledge-based measures and so on**\n",
    "   - Focus on more convinient and better way to preprocess the textual data by minimizing loss of information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "- https://medium.com/@adriensieg/text-similarities-da019229c894\n",
    "- http://article.nadiapub.com/IJDTA/vol10_no2/2.pdf\n",
    "- https://www.machinelearningplus.com/nlp/cosine-similarity/\n",
    "- https://github.com/makcedward/nlp/blob/master/sample/nlp-3_basic_distance_measurement_in_text_mining.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "224390953ffe4ffc953d2425899b26f28a6cfac249dfac765b546800b74315c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
